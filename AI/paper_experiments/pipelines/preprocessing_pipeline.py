"""
3ë‹¨ê³„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ + MNIST ë¶„ë¥˜ ëª¨ë¸ ì‹¤í—˜ ëª¨ë“ˆ

ë‹¨ê³„:
1. ì ˆë°˜ í¬ë¡­: YOLOë¡œ ë‹µì•ˆ ì˜ì—­ ì¶”ì¶œ (ì´ë¯¸ ì™„ë£Œ - yolo_answer_images ì‚¬ìš©)
2. ìˆ˜í‰ í¬ë¡­: ìˆ˜í‰ì„  ê¸°ì¤€ìœ¼ë¡œ í–‰ ë¶„í• 
3. í…ìŠ¤íŠ¸ í¬ë¡­: ê°œë³„ ìˆ«ì ë¸”ë¡ ë¶„í• 
4. ë‹¨ì¼ ìˆ«ì ì¸ì‹: MNIST ê¸°ë°˜ Vision Transformer
"""

import cv2
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from PIL import Image, ImageDraw
from transformers import pipeline
import sys
import os

# answer_recognition ëª¨ë“ˆì˜ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
answer_recognition_path = Path(__file__).parent.parent / "answer_recognition"
sys.path.append(str(answer_recognition_path))

from config import (
    YOLO_ANSWER_IMAGES_DIR, RESULTS_DIR, VISUALIZATION_DIR,
    BBOX_COLOR, BBOX_THICKNESS, TEXT_COLOR, TEXT_FONT, TEXT_SCALE, TEXT_THICKNESS
)

# MNIST ëª¨ë¸ ì´ˆê¸°í™”
try:
    mnist_model = pipeline("image-classification", 
                          model="farleyknight/mnist-digit-classification-2022-09-04", 
                          device=-1)  # CPU ì‚¬ìš©
    print("âœ… MNIST ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ MNIST ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    mnist_model = None

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
PIPELINE_RESULTS_DIR = RESULTS_DIR / "pipeline_results"
PIPELINE_VISUALIZATION_DIR = RESULTS_DIR / "pipeline_visualizations"


class PreprocessingPipeline:
    """3ë‹¨ê³„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.mnist_model = mnist_model
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        PIPELINE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        PIPELINE_VISUALIZATION_DIR.mkdir(parents=True, exist_ok=True)
    
    def step2_horizontal_crop(self, answer_image: Image.Image) -> List[Dict[str, Any]]:
        """
        2ë‹¨ê³„: ìˆ˜í‰ í¬ë¡­ - ìˆ˜í‰ì„  ê¸°ì¤€ìœ¼ë¡œ í–‰ ë¶„í• 
        
        Args:
            answer_image: YOLOë¡œ ì¶”ì¶œëœ ë‹µì•ˆ ì´ë¯¸ì§€
            
        Returns:
            ë¼ì¸ë³„ ì´ë¯¸ì§€ì™€ ìœ„ì¹˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print("    ğŸ”„ 2ë‹¨ê³„: ìˆ˜í‰ì„  ê²€ì¶œ ë° í–‰ ë¶„í• ")
        
        # PILì„ OpenCVë¡œ ë³€í™˜
        cv_image = cv2.cvtColor(np.array(answer_image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # ìˆ˜í‰ì„  ê²€ì¶œì„ ìœ„í•œ ì „ì²˜ë¦¬
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]
        
        # ìˆ˜í‰ ì»¤ë„ì„ ì‚¬ìš©í•œ í˜•íƒœí•™ì  ì—°ì‚°
        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))
        horizontal_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel)
        
        # ìœ¤ê³½ì„  ê²€ì¶œ
        contours, _ = cv2.findContours(horizontal_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # ìˆ˜í‰ì„  ì •ë³´ ì¶”ì¶œ
        line_info = []
        min_width = answer_image.width * 0.2  # ìµœì†Œ ì„  ê¸¸ì´
        
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            if w > min_width:  # ì¶©ë¶„íˆ ê¸´ ì„ ë§Œ ì„ íƒ
                line_info.append({'y': y, 'height': h})
        
        # y ì¢Œí‘œ ê¸°ì¤€ ì •ë ¬
        line_info.sort(key=lambda x: x['y'])
        
        # ê°€ê¹Œìš´ ì„ ë“¤ ë³‘í•©
        merged_lines = []
        for line in line_info:
            if not merged_lines or line['y'] - merged_lines[-1]['y'] > 15:
                merged_lines.append(line)
        
        print(f"      ğŸ” {len(merged_lines)}ê°œì˜ ìˆ˜í‰ì„  ê²€ì¶œë¨")
        
        # ë¼ì¸ ê°„ ì˜ì—­ ë¶„í• 
        line_crops = []
        image_height = answer_image.height
        
        for i in range(len(merged_lines) + 1):
            if i == 0:
                # ì²« ë²ˆì§¸ ë¼ì¸ ìœ„ìª½
                y_start = 0
                y_end = merged_lines[0]['y'] if merged_lines else image_height
            elif i == len(merged_lines):
                # ë§ˆì§€ë§‰ ë¼ì¸ ì•„ë˜ìª½
                y_start = merged_lines[-1]['y'] + merged_lines[-1]['height']
                y_end = image_height
            else:
                # ë¼ì¸ ì‚¬ì´ ì˜ì—­
                y_start = merged_lines[i-1]['y'] + merged_lines[i-1]['height']
                y_end = merged_lines[i]['y']
            
            # ìœ íš¨í•œ ë†’ì´ì¸ì§€ í™•ì¸
            if y_end - y_start > 20:  # ìµœì†Œ ë†’ì´
                line_crop = answer_image.crop((0, y_start, answer_image.width, y_end))
                line_crops.append({
                    'image': line_crop,
                    'y_start': y_start,
                    'y_end': y_end,
                    'line_index': len(line_crops)
                })
        
        print(f"      âœ… {len(line_crops)}ê°œì˜ í–‰ìœ¼ë¡œ ë¶„í•  ì™„ë£Œ")
        return line_crops
    
    def step3_text_crop(self, line_image: Image.Image, y_offset: int) -> List[Dict[str, Any]]:
        """
        3ë‹¨ê³„: í…ìŠ¤íŠ¸ í¬ë¡­ - ê°œë³„ ìˆ«ì ë¸”ë¡ ë¶„í• 
        
        Args:
            line_image: í–‰ë³„ ì´ë¯¸ì§€
            y_offset: ì›ë³¸ ì´ë¯¸ì§€ì—ì„œì˜ y ì˜¤í”„ì…‹
            
        Returns:
            í…ìŠ¤íŠ¸ ë¸”ë¡ë³„ ì´ë¯¸ì§€ì™€ ìœ„ì¹˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        # PILì„ OpenCVë¡œ ë³€í™˜
        cv_image = cv2.cvtColor(np.array(line_image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # ì´ì§„í™”
        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # ìœ¤ê³½ì„  ê²€ì¶œ
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ ë° í•„í„°ë§
        bboxes = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            if w > 5 and h > 5:  # ìµœì†Œ í¬ê¸° í•„í„°ë§
                bboxes.append((x, y, w, h))
        
        # x ì¢Œí‘œ ê¸°ì¤€ ì •ë ¬
        bboxes.sort(key=lambda box: box[0])
        
        # ê°€ê¹Œìš´ ë°•ìŠ¤ë“¤ ë³‘í•©
        merged_bboxes = []
        merge_distance = 50
        
        for bbox in bboxes:
            x, y, w, h = bbox
            
            # ê¸°ì¡´ ë°•ìŠ¤ì™€ ë³‘í•© ê°€ëŠ¥í•œì§€ í™•ì¸
            merged = False
            for i, (mx, my, mw, mh) in enumerate(merged_bboxes):
                if abs(x - (mx + mw)) < merge_distance:  # ê°€ê¹Œìš´ ê±°ë¦¬
                    # ë³‘í•©
                    new_x = min(x, mx)
                    new_y = min(y, my)
                    new_w = max(x + w, mx + mw) - new_x
                    new_h = max(y + h, my + mh) - new_y
                    merged_bboxes[i] = (new_x, new_y, new_w, new_h)
                    merged = True
                    break
            
            if not merged:
                merged_bboxes.append(bbox)
        
        # í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ
        text_crops = []
        for i, (x, y, w, h) in enumerate(merged_bboxes):
            # ì—¬ë°± ì¶”ê°€
            padding = 5
            crop_x = max(0, x - padding)
            crop_y = max(0, y - padding)
            crop_w = min(line_image.width - crop_x, w + 2 * padding)
            crop_h = min(line_image.height - crop_y, h + 2 * padding)
            
            text_crop = line_image.crop((crop_x, crop_y, crop_x + crop_w, crop_y + crop_h))
            
            text_crops.append({
                'image': text_crop,
                'x_in_line': crop_x,
                'y_in_line': crop_y,
                'width': crop_w,
                'height': crop_h,
                'x_in_answer': crop_x,  # ë‹µì•ˆ ì´ë¯¸ì§€ ê¸°ì¤€ xì¢Œí‘œ
                'y_in_answer': y_offset + crop_y,  # ë‹µì•ˆ ì´ë¯¸ì§€ ê¸°ì¤€ yì¢Œí‘œ
                'text_index': i
            })
        
        return text_crops
    
    def step4_digit_recognition(self, text_image: Image.Image) -> List[Dict[str, Any]]:
        """
        4ë‹¨ê³„: ë‹¨ì¼ ìˆ«ì ì¸ì‹ - MNIST ê¸°ë°˜ ë¶„ë¥˜
        
        Args:
            text_image: í…ìŠ¤íŠ¸ ë¸”ë¡ ì´ë¯¸ì§€
            
        Returns:
            ì¸ì‹ëœ ìˆ«ìë“¤ê³¼ ìœ„ì¹˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        if not self.mnist_model:
            return []
        
        # PILì„ OpenCVë¡œ ë³€í™˜í•˜ì—¬ ìœ¤ê³½ì„  ê²€ì¶œ
        cv_image = cv2.cvtColor(np.array(text_image.convert('L')), cv2.COLOR_GRAY2BGR)
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # ì´ì§„í™”
        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # ê°œë³„ ìˆ«ì ìœ¤ê³½ì„  ê²€ì¶œ
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        digit_results = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # í¬ê¸° í•„í„°ë§
            if w < 5 or h < 5:
                continue
            
            # ê°œë³„ ìˆ«ì ì´ë¯¸ì§€ ì¶”ì¶œ
            digit_image = text_image.convert('L').crop((x, y, x + w, y + h))
            
            # 28x28ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (MNIST í˜•ì‹)
            digit_image = digit_image.resize((28, 28), Image.Resampling.LANCZOS)
            
            try:
                # MNIST ëª¨ë¸ë¡œ ì˜ˆì¸¡
                prediction = self.mnist_model(digit_image)
                if prediction and len(prediction) > 0:
                    predicted_digit = prediction[0]['label']
                    confidence = prediction[0]['score']
                    
                    digit_results.append({
                        'digit': predicted_digit,
                        'confidence': confidence,
                        'bbox': (x, y, w, h),
                        'x_center': x + w // 2,
                        'y_center': y + h // 2
                    })
            except Exception as e:
                print(f"        âš ï¸ ìˆ«ì ì¸ì‹ ì‹¤íŒ¨: {e}")
                continue
        
        # x ì¢Œí‘œ ê¸°ì¤€ ì •ë ¬
        digit_results.sort(key=lambda d: d['x_center'])
        
        return digit_results
    
    def process_single_answer_image(self, image_path: Path) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ë‹µì•ˆ ì´ë¯¸ì§€ì— ëŒ€í•´ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            image_path: YOLOë¡œ ì¶”ì¶œëœ ë‹µì•ˆ ì´ë¯¸ì§€ ê²½ë¡œ
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            # ë‹µì•ˆ ì´ë¯¸ì§€ ë¡œë“œ
            answer_image = Image.open(image_path).convert('RGB')
            
            print(f"  ğŸ“– ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {image_path.name}")
            print(f"    ğŸ“ ë‹µì•ˆ ì´ë¯¸ì§€ í¬ê¸°: {answer_image.size}")
            
            # 2ë‹¨ê³„: ìˆ˜í‰ í¬ë¡­
            line_crops = self.step2_horizontal_crop(answer_image)
            
            # 3-4ë‹¨ê³„: ê° í–‰ì— ëŒ€í•´ í…ìŠ¤íŠ¸ í¬ë¡­ ë° ìˆ«ì ì¸ì‹
            all_text_crops = []
            all_digit_results = []
            
            for line_data in line_crops:
                print(f"      ğŸ”„ 3ë‹¨ê³„: í–‰ {line_data['line_index']} í…ìŠ¤íŠ¸ í¬ë¡­")
                
                text_crops = self.step3_text_crop(line_data['image'], line_data['y_start'])
                print(f"        ğŸ“¦ {len(text_crops)}ê°œì˜ í…ìŠ¤íŠ¸ ë¸”ë¡ ê²€ì¶œ")
                
                for text_data in text_crops:
                    print(f"          ğŸ”„ 4ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¸”ë¡ {text_data['text_index']} ìˆ«ì ì¸ì‹")
                    
                    digit_results = self.step4_digit_recognition(text_data['image'])
                    
                    # ì¢Œí‘œ ë³´ì • (ë‹µì•ˆ ì´ë¯¸ì§€ ê¸°ì¤€)
                    for digit in digit_results:
                        digit['x_in_answer'] = text_data['x_in_answer'] + digit['bbox'][0]
                        digit['y_in_answer'] = text_data['y_in_answer'] + digit['bbox'][1]
                    
                    # ê²°ê³¼ ì €ì¥
                    text_result = {
                        'text_crop_info': text_data,
                        'digit_results': digit_results,
                        'line_index': line_data['line_index']
                    }
                    
                    all_text_crops.append(text_result)
                    all_digit_results.extend(digit_results)
                    
                    print(f"            âœ… {len(digit_results)}ê°œ ìˆ«ì ì¸ì‹: " + 
                          " ".join([d['digit'] for d in digit_results]))
            
            processing_time = time.time() - start_time
            
            # ì „ì²´ ê²°ê³¼ ì¡°í•© (ì™¼ìª½â†’ì˜¤ë¥¸ìª½ ìˆœì„œ)
            all_digit_results.sort(key=lambda d: (d['y_in_answer'], d['x_in_answer']))
            final_result = "".join([d['digit'] for d in all_digit_results 
                                  if d['confidence'] > 0.7])
            
            print(f"    âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: '{final_result}' (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ)")
            
            return {
                "image_path": str(image_path),
                "image_name": image_path.name,
                "processing_time": processing_time,
                "line_crops": len(line_crops),
                "text_crops": len(all_text_crops),
                "total_digits": len(all_digit_results),
                "final_result": final_result,
                "digit_details": all_digit_results,
                "text_crop_details": all_text_crops,
                "success": True
            }
            
        except Exception as e:
            print(f"    âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            return {
                "image_path": str(image_path),
                "image_name": image_path.name,
                "error": str(e),
                "success": False
            }
    
    def visualize_pipeline_results(self, image_path: Path, result: Dict[str, Any]) -> Optional[Path]:
        """
        íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ ë‹µì•ˆ ì´ë¯¸ì§€ì— ì‹œê°í™”
        
        Args:
            image_path: ì›ë³¸ ë‹µì•ˆ ì´ë¯¸ì§€ ê²½ë¡œ
            result: íŒŒì´í”„ë¼ì¸ ê²°ê³¼
            
        Returns:
            ì‹œê°í™”ëœ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
        """
        try:
            # ë‹µì•ˆ ì´ë¯¸ì§€ ë¡œë“œ
            answer_image = cv2.imread(str(image_path))
            if answer_image is None:
                return None
            
            vis_image = answer_image.copy()
            
            if result.get("success", False) and result.get("digit_details", []):
                # ê° ì¸ì‹ëœ ìˆ«ìì— ëŒ€í•´ ë°”ìš´ë”© ë°•ìŠ¤ì™€ ê²°ê³¼ í‘œì‹œ
                for digit in result["digit_details"]:
                    x = digit['x_in_answer']
                    y = digit['y_in_answer']
                    w, h = digit['bbox'][2], digit['bbox'][3]
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)
                    cv2.rectangle(vis_image, (x, y), (x + w, y + h), BBOX_COLOR, BBOX_THICKNESS)
                    
                    # ì¸ì‹ ê²°ê³¼ì™€ ì‹ ë¢°ë„ í‘œì‹œ
                    label = f"{digit['digit']} ({digit['confidence']:.2f})"
                    
                    # í…ìŠ¤íŠ¸ ë°°ê²½
                    (text_width, text_height), baseline = cv2.getTextSize(
                        label, TEXT_FONT, TEXT_SCALE, TEXT_THICKNESS
                    )
                    
                    cv2.rectangle(
                        vis_image,
                        (x, y - text_height - baseline - 5),
                        (x + text_width, y),
                        (0, 0, 0),
                        -1
                    )
                    
                    # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
                    cv2.putText(
                        vis_image,
                        label,
                        (x, y - baseline - 2),
                        TEXT_FONT,
                        TEXT_SCALE,
                        TEXT_COLOR,
                        TEXT_THICKNESS
                    )
            
            # í•˜ë‹¨ì— ê²°ê³¼ ìš”ì•½ ì •ë³´ í‘œì‹œ
            info_lines = [
                f"Pipeline Result: {result.get('final_result', 'N/A')}",
                f"Digits: {result.get('total_digits', 0)}, Time: {result.get('processing_time', 0):.2f}s"
            ]
            
            y_pos = vis_image.shape[0] - 40
            for line in info_lines:
                cv2.putText(
                    vis_image,
                    line,
                    (10, y_pos),
                    TEXT_FONT,
                    TEXT_SCALE,
                    (255, 255, 255),  # í°ìƒ‰
                    TEXT_THICKNESS
                )
                y_pos += 25
            
            # ì €ì¥
            output_path = PIPELINE_VISUALIZATION_DIR / f"{image_path.stem}_pipeline_vis.jpg"
            cv2.imwrite(str(output_path), vis_image)
            
            return output_path
            
        except Exception as e:
            print(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def run_pipeline_experiment(self, input_dir: Path = None) -> List[Dict[str, Any]]:
        """
        ëª¨ë“  ë‹µì•ˆ ì´ë¯¸ì§€ì— ëŒ€í•´ íŒŒì´í”„ë¼ì¸ ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            input_dir: ì…ë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: YOLO_ANSWER_IMAGES_DIR)
            
        Returns:
            ëª¨ë“  ì´ë¯¸ì§€ì˜ íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        input_dir = input_dir or YOLO_ANSWER_IMAGES_DIR
        
        if not input_dir.exists():
            print(f"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_dir}")
            return []
        
        # ë‹µì•ˆ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
        image_files = list(input_dir.glob("*.jpg")) + list(input_dir.glob("*.png"))
        
        if not image_files:
            print(f"âŒ ë‹µì•ˆ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
            return []
        
        print(f"ğŸ“ ì´ {len(image_files)}ê°œì˜ ë‹µì•ˆ ì´ë¯¸ì§€ ë°œê²¬")
        print("=" * 60)
        print("3ë‹¨ê³„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ + MNIST ë¶„ë¥˜ ì‹¤í—˜ ì‹œì‘")
        print("=" * 60)
        
        all_results = []
        total_processing_time = 0
        
        for i, image_path in enumerate(image_files, 1):
            print(f"\n[{i}/{len(image_files)}] ì²˜ë¦¬ ì¤‘: {image_path.name}")
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            result = self.process_single_answer_image(image_path)
            all_results.append(result)
            
            if result.get("success", False):
                processing_time = result.get("processing_time", 0)
                total_processing_time += processing_time
                
                # ì‹œê°í™”
                vis_path = self.visualize_pipeline_results(image_path, result)
                if vis_path:
                    print(f"  ğŸ¨ ì‹œê°í™” ì €ì¥: {vis_path.name}")
                
            else:
                error_msg = result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                print(f"  âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {error_msg}")
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("3ë‹¨ê³„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ + MNIST ë¶„ë¥˜ ì‹¤í—˜ ì™„ë£Œ")
        print("=" * 60)
        
        successful_results = [r for r in all_results if r.get("success", False)]
        total_digits = sum(r.get("total_digits", 0) for r in successful_results)
        avg_processing_time = total_processing_time / len(successful_results) if successful_results else 0
        
        print(f"âœ… ì„±ê³µí•œ ì´ë¯¸ì§€: {len(successful_results)}/{len(image_files)}")
        print(f"ğŸ¯ ì´ ì¸ì‹ ìˆ«ì: {total_digits}")
        print(f"â±ï¸  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
        print(f"ğŸ“‚ ì‹œê°í™” ê²°ê³¼: {PIPELINE_VISUALIZATION_DIR}")
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        result_file = PIPELINE_RESULTS_DIR / "pipeline_results.json"
        
        def json_serializer(obj):
            """NumPy ë°ì´í„° íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
            if hasattr(obj, 'item'):
                return obj.item()
            elif hasattr(obj, 'tolist'):
                return obj.tolist()
            raise TypeError(f'Object of type {type(obj)} is not JSON serializable')
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False, default=json_serializer)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {result_file}")
        
        return all_results


def main():
    """ë©”ì¸ í•¨ìˆ˜ - 3ë‹¨ê³„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í—˜ ì‹¤í–‰"""
    pipeline = PreprocessingPipeline()
    results = pipeline.run_pipeline_experiment()
    
    if results:
        print(f"\nğŸ‰ ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì‹¤í—˜ ì™„ë£Œ! ì´ {len(results)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬")
    else:
        print(f"\nâŒ ì‹¤í—˜í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main() 